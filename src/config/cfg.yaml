# Model configuration
model_name: "roberta-base"            # Pre-trained model to use (e.g., "roberta-base", "bert-base-uncased")
output_dir: "./pretrained"      # Directory to save the fine-tuned model

# Training configuration
learning_rate: 2e-5                   # Learning rate for the optimizer
evaluation_strategy: "epoch"          # Evaluation strategy (e.g., "steps", "epoch")
per_device_train_batch_size: 64       # Batch size for training
per_device_eval_batch_size: 64        # Batch size for evaluation
num_train_epochs: 3                   # Number of training epochs
weight_decay: 0.01                    # Weight decay for regularization
save_strategy: "epoch"                  # Maximum number of checkpoints to save
warmup_steps: 500                     # Number of warmup steps for learning rate scheduler
dataloader_num_workers: 10            # Number of workers for data loading
fp16: True                            # Enable mixed precision training
load_best_model_at_end: True  # Load the best model at the end

# Validation configuration
metric: "squad"                        # Metric to use for validation (e.g., "squad", "glue")
eval_on_start: True                    # Perform evaluation at the start of training

# Dataset configuration
dataset_name: "minh21/COVID-QA-question-answering-biencoder-data-75_25"  # Hugging Face dataset name
max_length: 512                      # Maximum sequence length for tokenization
stride: 128                          # Stride for splitting long documents into chunks

# Logging configuration
logging_strategy: "epoch"             # Logging strategy (e.g., "steps", "epoch")
logging_dir: "./logs"                # Directory to save logs

# Evaluation configuration
n_best: 20                           # Number of best predictions to generate
max_answer_length: 30                # Maximum length of predicted answers

# Device
device: "cuda:0"                      # Device to use for training (e.g., "cuda:0", "CPU")
